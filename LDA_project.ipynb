{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramètres à estimer :\n",
    "1) hyperparamètres : \n",
    "    - M nombre de documents\n",
    "    - K nombre de topics\n",
    "    - W taille du vocabulaire\n",
    "    - Psi (décrit en dessous)\n",
    "    - N[i] nombre de mots par document, i allant de 1 à M (peut suivre une loi de Poisson de paramètre Psi)\n",
    "    - alpha : vecteur de dimension K correspondant au paramètre de la loi de Dirichlet (cf. exemple des ficelles, où les ficelles correspondent aux documents et les topics à leur découpe. alpha est alors le paramètre qui donne les proportions moyennes de chaque topic pour chaque document. alpha est le prior de Dirichlet placé sur theta\n",
    "    - beta : prior de Dirichlet placé sur fi\n",
    "    - fi : matrice de taille K*W, où fi[i,j] désigne la probabilité d'un mot w[j] d'appartenir au topic z[i]\n",
    "   \n",
    "2) paramètres latents (qui dépendent des hyperparamètres) : pour chaque document :\n",
    "    - theta : proportion exacte des topics dans chaque document (de dimension M*K)\n",
    "    - z : topics associés à chaque mot (de dimension N[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation des paramètres : on cherche à estimer z, theta et fi. On suppose que alpha et beta sont connus, ainsi que M, K, W, Psi et les N[i]\n",
    "    3 méthodes :\n",
    "        a) variational Bayes (VB)\n",
    "        b) expectation propagation\n",
    "        c) collapse Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Samir\\\\Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LDA_Model(object):\n",
    "    def __init__(self, K, alpha, beta):\n",
    "                 #, fi, theta, z, clean_corp, unique_corpus, n_jkw):\n",
    "        \"\"\"constructor\n",
    "        self, M, W, Psi, N, alpha, beta : les paramètres fixes de notre modèle\"\"\"\n",
    "        import numpy as np\n",
    "        import math\n",
    "        #Paramètres fixes du modèle\n",
    "        self.M = 0\n",
    "        self.K = K\n",
    "        self.W = 0\n",
    "        #self.Psi = Psi\n",
    "        #nombre de mots par document\n",
    "        self.N = np.zeros(self.M)\n",
    "        #paramètre de Dirichlet sur theta\n",
    "        #self.alpha = np.zeros(K)\n",
    "        self.alpha = alpha\n",
    "        #paramètre de Dirichlet sur fi\n",
    "        self.beta = beta\n",
    "        #theta\n",
    "        self.theta = np.zeros((self.M, K))\n",
    "        #fi\n",
    "        self.fi = np.zeros((K,self.W))\n",
    "        #z\n",
    "        self.z = [[] for _ in range(self.M)]\n",
    "        #on définit aussi un corpus nettoyé vide\n",
    "        self.clean_corp = []\n",
    "        #on définit les uniques mots du corpus pour chaque document\n",
    "        self.unique_corpus = [[] for _ in range(self.M)]\n",
    "        #le corpus entier en 1 liste\n",
    "        self.unique_flattened_corpus = []\n",
    "        #self.n_jkw = [[[] for k in range(self.K)] for j in range(self.M)]\n",
    "        self.n_jkw = [[{} for k in range(self.K)] for j in range(self.M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import glob\n",
    "\n",
    "\n",
    "#corpus représente le corpus de documents dont on cherche à appliquer la LDA\n",
    "#on va d'abord nettoyer le corpus\n",
    "def clean_corpus(self, corpus):\n",
    "    clean_corp = self.clean_corp\n",
    "    files = glob.glob(corpus)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    porter = PorterStemmer()\n",
    "#     snowball = SnowballStemmer('english')\n",
    "#     wordnet = WordNetLemmatizer()\n",
    "#on itère sur le corpus pour récupérer tous les fichiers\n",
    "    rootdir = 'BBC'\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for fle in files:\n",
    "       #on lit tous les fichiers\n",
    "            with open(subdir+\"\\\\\"+fle) as f:\n",
    "                text = f.read()\n",
    "                #pour chaque document du corpus, on effectue une tokenisation\n",
    "                tokenized_text = word_tokenize(text)\n",
    "                #on supprime la ponctuation\n",
    "                tokenized_docs_no_punctuation = []\n",
    "                for token in tokenized_text: \n",
    "                    new_token = regex.sub(u'', token)\n",
    "                    if not new_token == u'':\n",
    "                        tokenized_docs_no_punctuation.append(new_token)\n",
    "                #on supprime les \"stop words\"\n",
    "                tokenized_docs_no_stopwords = []\n",
    "                for word in tokenized_docs_no_punctuation:\n",
    "                    if not word in stopwords.words('english'):\n",
    "                        tokenized_docs_no_stopwords.append(word)\n",
    "                #enfin, on concatène les mots ayant des similarités de sens\n",
    "                preprocessed_docs = []\n",
    "                for word in tokenized_docs_no_stopwords:\n",
    "                    preprocessed_docs.append(porter.stem(word))\n",
    "                #on supprime les mots de moins de 3 caractères\n",
    "                out_length_words = []\n",
    "                for word in preprocessed_docs:\n",
    "                    if(len(word)>3):\n",
    "                        out_length_words.append(word)\n",
    "                clean_corp.append(out_length_words)    \n",
    "        #on retourne le corpus nettoyé, qui est une liste de chaque document du corpus\n",
    "        #return clean_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ici, on va définir le nombre de documents M, le nombre de mots par documents N[i], ainsi que\n",
    "#la taille du vocabulaire W\n",
    "def define_hyperparams_M_N_W(self):\n",
    "    #on définit le nombre de documents M\n",
    "    self.M = len(self.clean_corp)\n",
    "    #vecteur représentant le nombre de mots par document\n",
    "    #self.N = N\n",
    "    clean_corp = self.clean_corp\n",
    "    #on définit les uniques mots du corpus par document\n",
    "    unique_corpus = [[] for _ in range(self.M)]\n",
    "    #maintenant on va définir la taille du vocabulaire\n",
    "    #on transforme le corpus en une liste\n",
    "    flattened_corpus = [y for x in self.clean_corp for y in x]\n",
    "    unique_flattened_corpus = self.unique_flattened_corpus\n",
    "    for i in flattened_corpus:\n",
    "        if i not in unique_flattened_corpus:\n",
    "            unique_flattened_corpus.append(i)\n",
    "    W = len(unique_flattened_corpus)\n",
    "    self.W = W\n",
    "    self.unique_flattened_corpus = unique_flattened_corpus\n",
    "    #on définit aussi les mots uniques par document\n",
    "    i1 = 0\n",
    "    for i in clean_corp:\n",
    "        for j in i:\n",
    "            if j not in unique_corpus[i1]:\n",
    "                unique_corpus[i1].append(j)\n",
    "        i1 = i1 + 1\n",
    "    self.unique_corpus = unique_corpus\n",
    "    self.W = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on définit maintenant alpha, le paramètre de la loi de Dirichlet sur theta\n",
    "#on suppose K, le nombre de topics, fixé\n",
    "def define_hyperparams_alpha(self):\n",
    "    K = self.K\n",
    "    alpha = np.random.dirichlet(np.ones(K),size=1)[0]\n",
    "    self.alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on définit aussi beta, le paramètre de la loi de Dirichlet sur fi\n",
    "def define_hyperparams_beta(self):\n",
    "    W = self.W\n",
    "    beta = np.random.dirichlet(np.ones(W),size=1)[0]\n",
    "    self.beta = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#maintenant, on doit assigner aléatoirement un topic à chaque mot de chaque document\n",
    "def define_z_init(self):\n",
    "    K = self.K\n",
    "    clean_corp = self.clean_corp\n",
    "    alpha = self.alpha\n",
    "    z = [[] for _ in range(self.M)]\n",
    "    for i in range(len(clean_corp)):\n",
    "        for j in range(len(clean_corp[i])):\n",
    "            #on génère une loi multinomiale selon les probabilités de Dirichlet de alpha\n",
    "            #on tire un élément et on associe le topic au mot\n",
    "            p = np.random.multinomial(1, alpha, size=1)\n",
    "            z[i].append(np.argmax(p))\n",
    "    self.z = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compte par mot, le nombre de mots pour chaque topic, pour chaque document\n",
    "#on obtient une matrice à 3 indices\n",
    "from collections import Counter\n",
    "from itertools import compress\n",
    "def compute_n_jkw(self):\n",
    "    #unique_corpus = self.unique_corpus\n",
    "    clean_corp = self.clean_corp\n",
    "    z = self.z\n",
    "    M = self.M\n",
    "    K = self.K\n",
    "    n_jkw = [[{} for k in range(self.K)] for j in range(self.M)]\n",
    "    #j document\n",
    "    for j in range(M):\n",
    "        #k topic\n",
    "        for k in range(K):\n",
    "            #on récupère d'abord les mots pour un document, pour un topic donnée\n",
    "            fil = [x in [k] for x in test.z[j]]\n",
    "            words_for_j_and_k = list(compress(clean_corp[j], fil))\n",
    "            #create a counter and then a dictionary of words in the document for a topic\n",
    "            dictionary = Counter(words_for_j_and_k)\n",
    "            n_jkw[j][k].update(dictionary)\n",
    "    self.n_jkw = n_jkw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute n_jk.-ij\n",
    "def compute_n_jk_without_ij(self, j, k, w):\n",
    "    n_jkw = self.n_jkw\n",
    "    if w in self.n_jkw[j][k]:\n",
    "        dict_n_jkw = n_jkw[j][k]\n",
    "        #global dict_without_ij\n",
    "        dict_without_ij = {key : dict_n_jkw[key] for key in dict_n_jkw if key != w}\n",
    "        return (sum(dict_without_ij.values()), len(dict_without_ij)+1)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute n_jk.-ij^2 pour avoir l'espérance de la variable au carré\n",
    "def compute_n_jk_without_ij2(self, j, k, w):\n",
    "    n_jkw = self.n_jkw\n",
    "    if w in self.n_jkw[j][k]:\n",
    "        dict_n_jkw = n_jkw[j][k]\n",
    "        #global dict_without_ij\n",
    "        dict_without_ij = {key : dict_n_jkw[key] for key in dict_n_jkw if key != w}\n",
    "        return (sum([i ** 2 for i in dict_without_ij.values()]), len(dict_without_ij)+1)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute l'espérance de n_jk.-ij par rapport à w\n",
    "def esperance_n_jk_without_ij(self, j, k, w):\n",
    "    if w in self.n_jkw[j][k]:\n",
    "        return compute_n_jk_without_ij(self, j, k, w)[0] / compute_n_jk_without_ij(self, j, k, w)[1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute l'espérance de n_jk.-ij^2\n",
    "def esperance_n_jk_without_ij2(self, j, k, w):\n",
    "    if w in self.n_jkw[j][k]:\n",
    "        return compute_n_jk_without_ij2(self, j, k, w)[0] / compute_n_jk_without_ij2(self, j, k, w)[1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#maintenant on compute la variance de n_jk.-ij\n",
    "def variance_n_jk_without_ij(self, j, k, w):\n",
    "    if w in self.n_jkw[j][k]:\n",
    "        return esperance_n_jk_without_ij2(self, j, k, w) - (esperance_n_jk_without_ij(self, j, k, w)*esperance_n_jk_without_ij(self, j, k, w))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute l'espérance de n_.k.-ij par rapport à w et par rapport à j\n",
    "def esperance_n_k_without_ij(self, j, k, w):\n",
    "    M = self.M\n",
    "    s = 0\n",
    "    for j1 in range(M):\n",
    "        if w in self.n_jkw[j1][k]:\n",
    "            s = s + esperance_n_jk_without_ij(self, j1, k, w)\n",
    "    s = s - esperance_n_jk_without_ij(self, j, k, w)\n",
    "    s = s / (M - 1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute l'espérance de n_.k.-ij^2\n",
    "def esperance_n_k_without_ij2(self, j, k, w):\n",
    "    M = self.M\n",
    "    s = 0\n",
    "    for j1 in range(M):\n",
    "        if w in self.n_jkw[j1][k]:\n",
    "            s = s + esperance_n_jk_without_ij2(self, j1, k, w)\n",
    "    s = s - esperance_n_jk_without_ij2(self, j, k, w)\n",
    "    s = s / (M - 1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute maintenant la variance de n_.k.-ij^2\n",
    "def variance_n_k_without_ij(self, j, k, w):\n",
    "    return esperance_n_k_without_ij2(self, j, k, w) - (esperance_n_k_without_ij(self, j, k, w)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute n_.kx[ij]-ij\n",
    "def compute_n_kxij_without_ij(self, j, k, w):\n",
    "    s = 0\n",
    "    M = self.M\n",
    "    n_jkw = self.n_jkw\n",
    "    #on itère selon tous les documents, sauf le j\n",
    "    for j1 in [x for x in range(M) if x != j]:\n",
    "        if w in self.n_jkw[j1][k]:\n",
    "        #on fait la somme de tous les valeurs ayant w comme clé dans le dictionnaire n_jkw[j1][k]\n",
    "            s = s + n_jkw[j1][k][w]\n",
    "        else:\n",
    "            s = 0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute n_.kx[ij]-ij pour avoir l'espérance de la variable au carré\n",
    "def compute_n_kxij_without_ij2(self, j, k, w):\n",
    "    s = 0\n",
    "    M = self.M\n",
    "    n_jkw = self.n_jkw\n",
    "    #on itère selon tous les documents, sauf le j\n",
    "    for j1 in [x for x in range(self.M) if x != j]:\n",
    "        if w in self.n_jkw[j1][k]:\n",
    "        #on fait la somme de tous les valeurs ayant w comme clé dans le dictionnaire n_jkw[j1][k]\n",
    "            s = s + n_jkw[j1][k][w]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute l'espérance de n_.kx[ij]-ij\n",
    "def esperance_n_kxij_without_ij(self, j, k, w):\n",
    "    M = self.M\n",
    "    return compute_n_kxij_without_ij(self, j, k, w) / (M-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute l'espérance de n_.kx[ij]-ij^2\n",
    "def esperance_n_kxij_without_ij2(self, j, k, w):\n",
    "    M = self.M\n",
    "    return compute_n_kxij_without_ij2(self, j, k, w) / (M-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on compute maintenant la variance de n_.kx[ij]-ij^2\n",
    "def variance_n_kxij_without_ij(self, j, k, w):\n",
    "    return esperance_n_kxij_without_ij2(self, j, k, w) - (esperance_n_kxij_without_ij(self, j, k, w)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test.n_jkw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loc_esp__n_jk_without_ij = [[[]*len(test.n_jkw[0][0]) for w in range(len(test.unique_corpus[j]))] for j in range(test.M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if 'busi' in test.n_jkw[0][0]:\n",
    "#                      loc_esp__n_jk_without_ij[0][0].append(esperance_n_jk_without_ij(test, 0, 0, 'boost'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2a168a30ea5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jkw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.n_jkw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialisation des paramètres en utilisant le Collapse Gibbs Sampling\n",
    "def collapse_Gibbs_sampling(self, nbiter, alpha, beta, K):\n",
    "    alpha = self.alpha\n",
    "    beta = self.beta\n",
    "    K = self.K\n",
    "    M = self.M\n",
    "    W = self.W\n",
    "    n_jkw = self.n_jkw\n",
    "    unique_corpus = self.unique_corpus\n",
    "    z = self.z\n",
    "    #on compute une première fois toutes les espérances requises\n",
    "    loc_esp__n_jk_without_ij = [[[] for w in range(len(unique_corpus[j]))] for j in range(M)]\n",
    "    loc_var__n_jk_without_ij = [[[] for w in range(len(unique_corpus[j]))] for j in range(M)]\n",
    "    loc_esp__n_kxij_without_ij = [[[] for w in range(len(unique_corpus[j]))] for j in range(M)]\n",
    "    loc_var__n_kxij_without_ij = [[[] for w in range(len(unique_corpus[j]))] for j in range(M)]\n",
    "    loc_esp__n_k_without_ij = [[[] for w in range(len(unique_corpus[j]))] for j in range(M)]\n",
    "    loc_var__n_k_without_ij = [[[] for w in range(len(unique_corpus[j]))] for j in range(M)]\n",
    "    \n",
    "    print(\"début de computation des espérances et variances\")\n",
    "    for j in range(M):\n",
    "        for w in unique_corpus[j]:\n",
    "            for k in range(K):\n",
    "                index_w = unique_corpus[j].index(w)\n",
    "                #on définit des variables locales\n",
    "                #if w in self.n_jkw[j][k]:\n",
    "                loc_esp__n_jk_without_ij[j][index_w].append(esperance_n_jk_without_ij(self, j, k, w))\n",
    "                loc_var__n_jk_without_ij[j][index_w].append(variance_n_jk_without_ij(self, j, k, w))\n",
    "                loc_esp__n_kxij_without_ij[j][index_w].append(esperance_n_kxij_without_ij(self, j, k, w))\n",
    "                loc_var__n_kxij_without_ij[j][index_w].append(variance_n_kxij_without_ij(self, j, k, w))\n",
    "                loc_esp__n_k_without_ij[j][index_w].append(esperance_n_k_without_ij(self, j, k, w))\n",
    "                loc_var__n_k_without_ij[j][index_w].append(variance_n_k_without_ij(self, j, k, w))\n",
    "#                 loc_esp__n_jk_without_ij[j][index_w].append(1)\n",
    "#                 loc_var__n_jk_without_ij[j][index_w].append(1)\n",
    "#                 loc_esp__n_kxij_without_ij[j][index_w].append(1)\n",
    "#                 loc_var__n_kxij_without_ij[j][index_w].append(1)\n",
    "#                 loc_esp__n_k_without_ij[j][index_w].append(1)\n",
    "#                 loc_var__n_k_without_ij[j][index_w].append(1)\n",
    "                \n",
    "    print(\"fin de computation des espérances et variances\")          \n",
    "    for i in range(nbiter):\n",
    "        print(i)\n",
    "        for j in range(M):\n",
    "            for w in unique_corpus[j]:\n",
    "            #for k in range(K):\n",
    "                p = np.zeros(K)\n",
    "                index_w = unique_corpus[j].index(w)\n",
    "                k_topic = z[j][index_w]\n",
    "                loc_esp__n_jk_without_ij[j][index_w][k_topic] -= 1/(len(n_jkw[j][k_topic]))\n",
    "                loc_var__n_jk_without_ij[j][index_w][k_topic] -= 1/(len(n_jkw[j][k_topic]))\n",
    "                loc_esp__n_kxij_without_ij[j][index_w][k_topic] -= 1/(M-1)\n",
    "                loc_var__n_kxij_without_ij[j][index_w][k_topic] -= 1/(M-1)\n",
    "                loc_esp__n_k_without_ij[j][index_w][k_topic] -= 1/((M-1)*(len(n_jkw[j][k_topic])))\n",
    "                loc_var__n_k_without_ij[j][index_w][k_topic] -= 1/((M-1)*(len(n_jkw[j][k_topic])))\n",
    "                        \n",
    "                a1 = (loc_var__n_jk_without_ij[j][index_w][k_topic])/(2*(0.1 + loc_esp__n_jk_without_ij[j][index_w][k_topic])**2)\n",
    "                a2 = (loc_var__n_kxij_without_ij[j][index_w][k_topic])/(2*(0.1 + loc_esp__n_kxij_without_ij[j][index_w][k_topic])**2)\n",
    "                a3 = (loc_var__n_k_without_ij[j][index_w][k_topic])/(2*(W*0.1 + loc_esp__n_k_without_ij[j][index_w][k_topic]))\n",
    "                exp = np.exp(-a1 -a2 +a3)\n",
    "                for k in range(K):\n",
    "                    p[k] = ((0.1 + loc_esp__n_jk_without_ij[j][index_w][k])*(0.1 + loc_esp__n_kxij_without_ij[j][index_w][k])/(0.1*W + loc_esp__n_k_without_ij[j][index_w][k]))*exp\n",
    "                #on normalise les p[k] calculés ci-dessus\n",
    "                p_normalized = p/sum(p)\n",
    "                #on définit alors une nouvelle multinomiale pour le mot w, de longueur K, pour choisir le nouveau topic \n",
    "                proba = np.random.multinomial(1, p_normalized, size=1)\n",
    "                #on détermine le nouveau topic qui a la plus forte probabilité pour le mot w\n",
    "                new_topic = np.argmax(proba)\n",
    "                #on remplace l'élément de z et on réactualise espérances et variances\n",
    "                z[j][index_w] = new_topic\n",
    "                loc_esp__n_jk_without_ij[j][index_w][new_topic] += 1/(len(n_jkw[j][new_topic]))\n",
    "                loc_var__n_jk_without_ij[j][index_w][new_topic] += 1/(len(n_jkw[j][new_topic]))\n",
    "                loc_esp__n_kxij_without_ij[j][index_w][new_topic] += 1/(M-1)\n",
    "                loc_var__n_kxij_without_ij[j][index_w][new_topic] += 1/(M-1)\n",
    "                loc_esp__n_k_without_ij[j][index_w][new_topic] += 1/((M-1)*(len(n_jkw[j][new_topic])))\n",
    "                loc_var__n_k_without_ij[j][index_w][new_topic] += 1/((M-1)*(len(n_jkw[j][new_topic])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on implémente alors les paramètres fi et theta\n",
    "#on compute d'abord toutes les espérances nécessaires\n",
    "#espérance de n_jk.\n",
    "def compute_n_jk(self, j, k):\n",
    "    n_jkw = self.n_jkw\n",
    "    dict_n_jkw = n_jkw[j][k]\n",
    "    return (sum(dict_n_jkw.values()), len(dict_n_jkw))\n",
    "\n",
    "def esperance_n_jk(self, j, k):\n",
    "    return compute_n_jk(self, j, k)[0] / compute_n_jk(self, j, k)[1]\n",
    "\n",
    "#espérance de n_.kw\n",
    "def compute_n_kxij(self, k, w):\n",
    "    s = 0\n",
    "    n_jkw = self.n_jkw\n",
    "    M = self.M\n",
    "    #for j1 in [x for x in range(len(n_jkw))]:\n",
    "    for j1 in range(M):\n",
    "        if w in self.n_jkw[j1][k]:\n",
    "            s = s + n_jkw[j1][k][w]\n",
    "    return s\n",
    "\n",
    "def esperance_n_kxij(self, k, w):\n",
    "    M = self.M\n",
    "    return compute_n_kxij(self, k, w) / M\n",
    "\n",
    "#espérance de n_.k.\n",
    "def esperance_n_k(self, k):\n",
    "    M = self.M\n",
    "    s = 0\n",
    "    for j1 in range(M):\n",
    "        s = s + esperance_n_jk(self, j1, k)\n",
    "    s = s / M\n",
    "    return s\n",
    "    \n",
    "#espérance de n_j..\n",
    "def esperance_n_j(self, j):\n",
    "    K = self.K\n",
    "    s = 0\n",
    "    for k1 in range(K):\n",
    "        s = s + esperance_n_jk(self, j, k1)\n",
    "    s = s / K\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#maintenant on compute theta et fi\n",
    "def compute_theta(self):\n",
    "    alpha = self.alpha\n",
    "    K = self.K\n",
    "    M = self.M\n",
    "    theta = np.zeros((self.M, K))\n",
    "    for j in range(M):\n",
    "        for k in range(K):\n",
    "            #theta[j,k] = (alpha[k] + esperance_n_jk(self, j, k)) / (K*alpha[k] + esperance_n_j(self, j))\n",
    "            theta[j,k] = (0.1 + esperance_n_jk(self, j, k)) / (K*0.1 + esperance_n_j(self, j))\n",
    "    self.theta = theta\n",
    "            \n",
    "def compute_fi(self):\n",
    "    beta = self.beta\n",
    "    W = self.W\n",
    "    K = self.K\n",
    "    unique_corpus = self.unique_corpus\n",
    "    unique_flattened_corpus = self.unique_flattened_corpus\n",
    "    fi = np.zeros((K,self.W))\n",
    "    for k in range(K):\n",
    "        for w1 in range(len(unique_flattened_corpus)):\n",
    "            w = unique_flattened_corpus[w1]\n",
    "            fi[k,w1] = (0.1 + esperance_n_kxij(self, k, w)) / (W*0.1 + esperance_n_k(self, k))\n",
    "    self.fi = fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on fait le test de l'algorithme\n",
    "test = LDA_Model(K = 5, alpha =0.1, beta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_corpus(test, \"corpus_test/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "define_hyperparams_M_N_W(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "define_hyperparams_alpha(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "define_hyperparams_beta(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "define_z_init(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_n_jkw(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "début de computation des espérances et variances\n"
     ]
    }
   ],
   "source": [
    "collapse_Gibbs_sampling(test, 100, test.alpha, test.beta, test.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_theta(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_fi(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testo = np.transpose(test.fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tall = np.zeros(len(testo))\n",
    "for i in range(len(testo)):\n",
    "    tall[i] = np.argmax(testo[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fil1 = [x in [0] for x in tall]\n",
    "topic1 = list(compress(test.unique_flattened_corpus, fil1))\n",
    "\n",
    "fil2 = [x in [1] for x in tall]\n",
    "topic2 = list(compress(test.unique_flattened_corpus, fil2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beavertail',\n",
       " 'sever',\n",
       " 'confect',\n",
       " 'whip',\n",
       " 'cream',\n",
       " 'sugar',\n",
       " 'hazelnut',\n",
       " 'oper',\n",
       " 'castor',\n",
       " 'stand',\n",
       " 'trademark',\n",
       " 'compani',\n",
       " 'bell',\n",
       " 'tree',\n",
       " 'metr',\n",
       " 'member',\n",
       " 'nativ',\n",
       " 'mexico',\n",
       " 'central',\n",
       " 'plant',\n",
       " 'mountain',\n",
       " '4900–5600',\n",
       " 'leav',\n",
       " 'dietari',\n",
       " 'peopl',\n",
       " 'pedro',\n",
       " 'tuber',\n",
       " 'herbac',\n",
       " 'grow',\n",
       " 'winter',\n",
       " 'develop',\n",
       " 'canelik',\n",
       " '4angl',\n",
       " 'stem',\n",
       " 'node',\n",
       " 'larg',\n",
       " 'pendant',\n",
       " '75150mm',\n",
       " 'floret',\n",
       " 'colour',\n",
       " 'speci',\n",
       " 'growth',\n",
       " 'link',\n",
       " 'hour',\n",
       " 'usual',\n",
       " 'first',\n",
       " 'least',\n",
       " 'horizont',\n",
       " 'brought',\n",
       " 'europ',\n",
       " '1823–1885',\n",
       " 'travel',\n",
       " 'later',\n",
       " '1872–73',\n",
       " 'rhizom',\n",
       " 'reach',\n",
       " 'rare',\n",
       " 'centimet',\n",
       " 'leaflet',\n",
       " 'ovat',\n",
       " '5–10',\n",
       " 'head',\n",
       " 'pink',\n",
       " 'purpl',\n",
       " 'fish',\n",
       " 'brewi',\n",
       " 'pronounc',\n",
       " 'brew',\n",
       " 'newfoundland',\n",
       " 'hard',\n",
       " 'with',\n",
       " 'abund',\n",
       " 'around',\n",
       " 'synonym',\n",
       " 'mani',\n",
       " 'delicaci',\n",
       " 'serv',\n",
       " 'vari',\n",
       " 'commun',\n",
       " 'ingredi',\n",
       " 'alway',\n",
       " 'typic',\n",
       " 'salt',\n",
       " 'soak',\n",
       " 'water',\n",
       " 'overnight',\n",
       " 'reduc',\n",
       " 'also',\n",
       " 'next',\n",
       " 'separ',\n",
       " 'tender',\n",
       " 'scrunchion',\n",
       " 'pork',\n",
       " 'piec',\n",
       " 'both',\n",
       " 'render',\n",
       " 'often',\n",
       " 'fresh',\n",
       " 'drawn',\n",
       " 'butter',\n",
       " 'sometim',\n",
       " 'instanc',\n",
       " 'melt',\n",
       " 'onion',\n",
       " 'thicken',\n",
       " 'flour',\n",
       " 'nova',\n",
       " 'plate',\n",
       " 'mound',\n",
       " 'carrot',\n",
       " 'smoke',\n",
       " 'made',\n",
       " 'hind',\n",
       " 'refer',\n",
       " 'world',\n",
       " 'includ',\n",
       " 'number',\n",
       " 'highli',\n",
       " 'varieti',\n",
       " 'jamón',\n",
       " 'product',\n",
       " 'term',\n",
       " 'control',\n",
       " 'area',\n",
       " 'addit',\n",
       " 'numer',\n",
       " 'geograph',\n",
       " 'protect',\n",
       " 'prosciutto',\n",
       " 'parma',\n",
       " 'smithfield',\n",
       " 'helianthemum',\n",
       " 'apenninum',\n",
       " 'rockros',\n",
       " 'grassi',\n",
       " 'rocki',\n",
       " 'part',\n",
       " 'march',\n",
       " 'stamen',\n",
       " 'outer',\n",
       " 'sepal',\n",
       " 'stripe',\n",
       " 'inner',\n",
       " 'lanceshap',\n",
       " 'opposit',\n",
       " 'pair',\n",
       " 'they',\n",
       " 'greyish',\n",
       " 'whitish',\n",
       " 'hair',\n",
       " 'especi',\n",
       " 'roll',\n",
       " 'japanes',\n",
       " 'slightli',\n",
       " 'avocado',\n",
       " 'salmon',\n",
       " 'tuna',\n",
       " 'crab',\n",
       " 'wedg',\n",
       " 'origin',\n",
       " 'debat',\n",
       " 'chef',\n",
       " 'restaur',\n",
       " 'inventor',\n",
       " 'wide',\n",
       " 'avail',\n",
       " 'becom',\n",
       " 'along',\n",
       " 'sandwich',\n",
       " 'toutin',\n",
       " 'type',\n",
       " 'canadian',\n",
       " 'commonli',\n",
       " 'produc',\n",
       " 'dark',\n",
       " 'molass',\n",
       " 'corn',\n",
       " 'syrup',\n",
       " 'brunch',\n",
       " 'still',\n",
       " 'find',\n",
       " 'fatback',\n",
       " 'consider',\n",
       " 'healthi',\n",
       " 'oliv',\n",
       " 'andor',\n",
       " 'canola']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pastri',\n",
       " 'similar',\n",
       " 'dough',\n",
       " 'choic',\n",
       " 'sweet',\n",
       " 'condiment',\n",
       " 'banana',\n",
       " 'slice',\n",
       " 'crumbl',\n",
       " 'oreo',\n",
       " 'cinnamon',\n",
       " 'chocol',\n",
       " 'canada',\n",
       " 'franchis',\n",
       " 'current',\n",
       " 'store',\n",
       " 'queue',\n",
       " 'worldwid',\n",
       " 'regist',\n",
       " 'sinc',\n",
       " '1988',\n",
       " 'affili',\n",
       " 'dahlia',\n",
       " 'imperiali',\n",
       " 'tall',\n",
       " 'genu',\n",
       " 'america',\n",
       " 'colombia',\n",
       " 'upland',\n",
       " 'occur',\n",
       " 'elev',\n",
       " '1500–1700',\n",
       " 'supplement',\n",
       " 'qeqchi',\n",
       " 'carchá',\n",
       " 'alta',\n",
       " 'verapaz',\n",
       " 'guatemala',\n",
       " 'perenni',\n",
       " 'rapidli',\n",
       " 'base',\n",
       " 'dormant',\n",
       " 'period',\n",
       " 'brittl',\n",
       " 'swollen',\n",
       " 'tripinn',\n",
       " 'near',\n",
       " 'ground',\n",
       " 'soon',\n",
       " 'shed',\n",
       " 'flowerhead',\n",
       " 'across',\n",
       " 'lavend',\n",
       " 'mauvishpink',\n",
       " 'fastgrow',\n",
       " 'spurt',\n",
       " 'shorter',\n",
       " 'daylight',\n",
       " 'come',\n",
       " 'flower',\n",
       " 'autumn',\n",
       " 'frost',\n",
       " 'propag',\n",
       " 'seed',\n",
       " 'long',\n",
       " 'laid',\n",
       " 'soil',\n",
       " 'some',\n",
       " '16th',\n",
       " 'centuri',\n",
       " 'describ',\n",
       " '1863',\n",
       " 'benedikt',\n",
       " 'roezl',\n",
       " 'great',\n",
       " 'czech',\n",
       " 'orchid',\n",
       " 'collector',\n",
       " 'year',\n",
       " 'went',\n",
       " 'odyssey',\n",
       " 'pinnata',\n",
       " 'root',\n",
       " 'height',\n",
       " 'erect',\n",
       " 'branch',\n",
       " 'infloresc',\n",
       " 'simpl',\n",
       " 'eight',\n",
       " 'diamet',\n",
       " 'length',\n",
       " 'deep',\n",
       " 'tradit',\n",
       " 'meal',\n",
       " 'consist',\n",
       " 'bread',\n",
       " 'tack',\n",
       " 'coast',\n",
       " 'labrador',\n",
       " 'becam',\n",
       " 'household',\n",
       " 'main',\n",
       " 'recip',\n",
       " 'even',\n",
       " 'primari',\n",
       " 'call',\n",
       " 'content',\n",
       " 'boil',\n",
       " 'togeth',\n",
       " 'small',\n",
       " 'liquid',\n",
       " 'drizzl',\n",
       " 'fisherman',\n",
       " 'chop',\n",
       " 'instead',\n",
       " 'mixtur',\n",
       " 'saucepan',\n",
       " 'scotia',\n",
       " 'dish',\n",
       " 'known',\n",
       " 'scrap',\n",
       " 'plain',\n",
       " 'potato',\n",
       " 'preserv',\n",
       " 'cure',\n",
       " 'swine',\n",
       " 'specif',\n",
       " 'covet',\n",
       " 'region',\n",
       " 'specialti',\n",
       " 'westphalian',\n",
       " 'spanish',\n",
       " 'technic',\n",
       " 'process',\n",
       " 'meat',\n",
       " 'mechan',\n",
       " 'reform',\n",
       " 'precis',\n",
       " 'natur',\n",
       " 'statut',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'european',\n",
       " 'union',\n",
       " 'name',\n",
       " 'toscano',\n",
       " 'white',\n",
       " 'whiteflow',\n",
       " 'rock',\n",
       " 'rose',\n",
       " 'found',\n",
       " 'place',\n",
       " 'juli',\n",
       " 'pentamer',\n",
       " 'yellow',\n",
       " 'centr',\n",
       " 'three',\n",
       " 'hairi',\n",
       " 'cluster',\n",
       " 'linear',\n",
       " 'cover',\n",
       " 'undersid',\n",
       " 'sushi',\n",
       " 'pizza',\n",
       " 'variant',\n",
       " 'northeastern',\n",
       " 'crispi',\n",
       " 'chewi',\n",
       " 'rice',\n",
       " 'patti',\n",
       " 'layer',\n",
       " 'blend',\n",
       " 'mayonnais',\n",
       " 'wasabi',\n",
       " 'powder',\n",
       " 'nori',\n",
       " 'pickl',\n",
       " 'ginger',\n",
       " 'side',\n",
       " 'atami',\n",
       " 'montreal',\n",
       " 'quebec',\n",
       " 'claim',\n",
       " '1992',\n",
       " 'popular',\n",
       " 'toronto',\n",
       " 'quickli',\n",
       " 'citi',\n",
       " 'signatur',\n",
       " 'peameal',\n",
       " 'bacon',\n",
       " 'touton',\n",
       " 'pancak',\n",
       " 'leftov',\n",
       " 'breakfast',\n",
       " 'quit',\n",
       " 'menu',\n",
       " 'local',\n",
       " 'much',\n",
       " 'rarer',\n",
       " 'cook',\n",
       " 'modern',\n",
       " 'seen',\n",
       " 'evolut',\n",
       " 'like',\n",
       " 'combin',\n",
       " 'clarifi',\n",
       " 'accompani']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
